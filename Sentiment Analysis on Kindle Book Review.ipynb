{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac4fd5a",
   "metadata": {
    "id": "0ac4fd5a"
   },
   "source": [
    "# Sentiment Analysis on Kindle Book Review\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69df055a",
   "metadata": {
    "id": "69df055a"
   },
   "outputs": [],
   "source": [
    "# Importing library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras_tuner as kt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, log_loss\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37462256",
   "metadata": {
    "id": "37462256"
   },
   "source": [
    "# 1. Read database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "TtWoo8-rhae5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TtWoo8-rhae5",
    "outputId": "7a47f3aa-c78d-45aa-d02b-aa271f102ffb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wongj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wongj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wongj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2985c09",
   "metadata": {
    "id": "e2985c09"
   },
   "outputs": [],
   "source": [
    "# loading  dataset\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "data_realtest = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d0b6c4",
   "metadata": {
    "id": "72d0b6c4"
   },
   "source": [
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Ww604J0j8myr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ww604J0j8myr",
    "outputId": "0877932a-904b-4140-9069-ee443ee45676"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training constants for padded_sequence, maxlen will be rounded off to 400\n",
    "data['reviewText'].apply(lambda x : len(x.split(' '))).quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "FOkxLxglbqvb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOkxLxglbqvb",
    "outputId": "ab8616b4-7f68-4c9f-84a6-1d3ea3cdf32a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 23419),\n",
       " ('book', 6382),\n",
       " ('The', 5047),\n",
       " ('story', 5018),\n",
       " ('read', 4328),\n",
       " ('one', 3465),\n",
       " ('like', 3431),\n",
       " ('This', 3002),\n",
       " ('would', 2913),\n",
       " ('really', 2655),\n",
       " ('It', 2574),\n",
       " ('good', 2518),\n",
       " ('love', 2222),\n",
       " ('get', 2207),\n",
       " ('characters', 2172),\n",
       " ('reading', 1766),\n",
       " ('first', 1747),\n",
       " ('much', 1736),\n",
       " ('books', 1682),\n",
       " ('author', 1668),\n",
       " ('even', 1643),\n",
       " ('-', 1605),\n",
       " ('could', 1605),\n",
       " ('time', 1571),\n",
       " ('little', 1543),\n",
       " ('book.', 1511),\n",
       " ('it.', 1360),\n",
       " ('short', 1318),\n",
       " ('well', 1304),\n",
       " ('two', 1301),\n",
       " ('great', 1271),\n",
       " ('know', 1260),\n",
       " ('way', 1232),\n",
       " ('think', 1210),\n",
       " (\"I'm\", 1185),\n",
       " ('story.', 1179),\n",
       " ('sex', 1138),\n",
       " ('enjoyed', 1097),\n",
       " ('series', 1087),\n",
       " ('find', 1069),\n",
       " ('never', 1069),\n",
       " ('also', 1069),\n",
       " ('make', 1058),\n",
       " ('see', 1022),\n",
       " ('want', 991),\n",
       " ('There', 975),\n",
       " ('She', 970),\n",
       " ('many', 964),\n",
       " ('character', 959),\n",
       " ('found', 955),\n",
       " ('read.', 949),\n",
       " ('He', 946),\n",
       " ('A', 941),\n",
       " ('plot', 938),\n",
       " ('going', 918),\n",
       " ('liked', 900),\n",
       " ('But', 881),\n",
       " ('got', 875),\n",
       " ('bit', 872),\n",
       " ('stories', 834),\n",
       " ('thought', 833),\n",
       " ('And', 827),\n",
       " ('If', 812),\n",
       " ('made', 811),\n",
       " ('still', 784),\n",
       " ('say', 773),\n",
       " ('give', 767),\n",
       " ('book,', 761),\n",
       " ('loved', 760),\n",
       " ('another', 758),\n",
       " ('something', 749),\n",
       " ('enough', 744),\n",
       " ('back', 739),\n",
       " ('romance', 739),\n",
       " ('go', 738),\n",
       " ('writing', 736),\n",
       " ('written', 734),\n",
       " (\"can't\", 731),\n",
       " ('lot', 720),\n",
       " ('people', 717)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show most frequent 80 words, used for manual pre-processing\n",
    "data_pre = data\n",
    "stop = set(stopwords.words('english'))\n",
    "data_pre['reviewText'] = data_pre['reviewText'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "from collections import Counter\n",
    "Counter(\" \".join(data_pre['reviewText']).split()).most_common(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b304efa9",
   "metadata": {
    "id": "b304efa9"
   },
   "outputs": [],
   "source": [
    "review = []\n",
    "my_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "my_stopwords.remove('no')\n",
    "my_stopwords.remove('not')\n",
    "my_stopwords.remove('very')\n",
    "my_stopwords.add('book')\n",
    "my_stopwords.add('story')\n",
    "my_stopwords.add('author')\n",
    "my_stopwords.add('read')\n",
    "my_stopwords.add('reading')\n",
    "my_stopwords.add('character')\n",
    "my_stopwords.add('I')\n",
    "my_stopwords.add('The')\n",
    "my_stopwords.add('This')\n",
    "my_stopwords.add('It')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vocabulary = {}\n",
    "review_size = []\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return nltk.tokenize.word_tokenize(nopunct)\n",
    "\n",
    "\n",
    "for i in range(len(data['rating'])):\n",
    "    review.append(data['summary'][i] +' '+ data['reviewText'][i])\n",
    "\n",
    "counts = Counter()\n",
    "\n",
    "for i in range(len(review)):\n",
    "    r = []\n",
    "    tokens = tokenize(review[i])\n",
    "    for t in tokens:\n",
    "        if t not in my_stopwords and len(t) > 1:\n",
    "            r.append(lemmatizer.lemmatize(t))\n",
    "            \n",
    "    review[i] = r\n",
    "    counts.update(r)\n",
    "    review_size.append(len(r))\n",
    "    \n",
    "data['review'] = review\n",
    "\n",
    "review = []\n",
    "review_size = []\n",
    "\n",
    "for i in range(len(data_realtest['Id'])):\n",
    "    review.append(data_realtest['summary'][i] +' '+ data_realtest['reviewText'][i])\n",
    "\n",
    "counts = Counter()\n",
    "\n",
    "for i in range(len(review)):\n",
    "    r = []\n",
    "    tokens = tokenize(review[i])\n",
    "    for t in tokens:\n",
    "        if t not in my_stopwords and len(t) > 1:\n",
    "            r.append(lemmatizer.lemmatize(t))\n",
    "            \n",
    "    review[i] = r\n",
    "    counts.update(r)\n",
    "    review_size.append(len(r))\n",
    "    \n",
    "data_realtest['review'] = review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e634d13a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e634d13a",
    "outputId": "7dd7c38a-3e32-4c74-ca10-c21e4ead7d59",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating        0\n",
       "reviewText    0\n",
       "summary       0\n",
       "review        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e81d33c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e81d33c3",
    "outputId": "d1756c41-38f0-4f37-df11-95e5821c3cc0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    2400\n",
       "5    2200\n",
       "1    1700\n",
       "2    1500\n",
       "3    1200\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f8b857",
   "metadata": {
    "id": "e6f8b857"
   },
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87d993b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87d993b9",
    "outputId": "0d3caedf-7eeb-4df2-b6a0-bc7e3a6e2c09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5400,), (1800,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into X and y\n",
    "X = data.review\n",
    "y = data.rating\n",
    "X_realtrain = X\n",
    "y_realtrain = y\n",
    "X_realtest = data_realtest.review\n",
    "\n",
    "\n",
    "# 60:20:20 split\n",
    "X_tv, X_test, y_tv, y_test = train_test_split(X,y,test_size=0.20,random_state=0)\n",
    "X_train, X_vali, y_train, y_vali = train_test_split(X_tv, y_tv, test_size = 1/4,random_state=0)\n",
    "\n",
    "y_train_array = np.array(y_train)\n",
    "y_vali_array = np.array(y_vali)\n",
    "y_tv_array = np.array(y_tv)\n",
    "y_test_array = np.array(y_test)\n",
    "y_realtrain_array = np.array(y_realtrain)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b38aae2",
   "metadata": {
    "id": "5b38aae2"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# One Hot Encode Y values:\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y_train = encoder.fit_transform(y_train.values)\n",
    "y_train = to_categorical(y_train) \n",
    "\n",
    "y_vali = encoder.fit_transform(y_vali.values)\n",
    "y_vali = to_categorical(y_vali) \n",
    "\n",
    "y_tv = encoder.fit_transform(y_tv.values)\n",
    "y_tv = to_categorical(y_tv) \n",
    "\n",
    "y_test = encoder.fit_transform(y_test.values)\n",
    "y_test = to_categorical(y_test) \n",
    "\n",
    "y_realtrain = encoder.fit_transform(y_realtrain.values)\n",
    "y_realtrain = to_categorical(y_realtrain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "316ac9aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "316ac9aa",
    "outputId": "6454211a-402a-4a6d-d13f-a29652dbf662",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 23727\n",
      "Max Token Index: 9999 \n",
      "\n",
      "Sample Before Processing: ['seriously', 'short', 'even', 'qualify', 'novella', 'guess', 'get', 'pay', 'free']\n",
      "Sample After Processing: ['seriously short even qualify novella guess get pay free'] \n",
      "\n",
      "What the model will interpret: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 477, 16, 17, 7467, 153, 343, 9, 454, 73]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, lower=True) # num_words:the maximum number of words to keep, based on word frequency\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_vali = tokenizer.texts_to_sequences(X_vali)\n",
    "sequences_tv = tokenizer.texts_to_sequences(X_tv)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "sequences_realtrain = tokenizer.texts_to_sequences(X)\n",
    "sequences_realtest = tokenizer.texts_to_sequences(X_realtest)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "padded_sequence_train = pad_sequences(sequences_train, maxlen=400) # maxlen, higher num takes longer to run\n",
    "padded_sequence_vali = pad_sequences(sequences_vali, maxlen=400)\n",
    "padded_sequence_tv = pad_sequences(sequences_tv, maxlen=400)\n",
    "padded_sequence_test = pad_sequences(sequences_test, maxlen=400)\n",
    "padded_sequence_realtrain = pad_sequences(sequences_realtrain, maxlen=400)\n",
    "padded_sequence_realtest = pad_sequences(sequences_realtest, maxlen=400)\n",
    "\n",
    "print('Number of Tokens:', len(tokenizer.word_index))\n",
    "print(\"Max Token Index:\", padded_sequence_train.max(), \"\\n\")\n",
    "\n",
    "print('Sample Before Processing:', X_train.values[0])\n",
    "print('Sample After Processing:', tokenizer.sequences_to_texts([padded_sequence_train[0]]), '\\n')\n",
    "\n",
    "print('What the model will interpret:', padded_sequence_train[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53fe2aa",
   "metadata": {
    "id": "f53fe2aa"
   },
   "source": [
    "# 4. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZrBcGdiP82Pl",
   "metadata": {
    "id": "ZrBcGdiP82Pl"
   },
   "source": [
    "## 4.1 Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8301af37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8301af37",
    "outputId": "5a6a45fc-ea2b-4243-a8e3-4f54fd691d8a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 27m 51s]\n",
      "val_accuracy: 0.4750000089406967\n",
      "\n",
      "Best val_accuracy So Far: 0.5016666650772095\n",
      "Total elapsed time: 00h 48m 30s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = vocab_size, output_dim = 128, input_length=400))\n",
    "    model.add(LSTM(units=hp.Int('units_LSTM',min_value=16,max_value=256,step=16)))\n",
    "    model.add(Dropout(hp.Float('rate', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    # Tune the number of dense layers\n",
    "    for i in range(hp.Int('num_layers', 0, 3)):\n",
    "        model.add(Dense(units=hp.Int('units_'+str(i), min_value=16, max_value=256, step=16), activation=\"relu\"))    \n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3])\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(build_model,objective=\"val_accuracy\", max_trials=3,executions_per_trial=2,overwrite=True)\n",
    "\n",
    "tuner.search(padded_sequence_train, y_train, validation_data=(padded_sequence_vali, y_vali),\n",
    "                    epochs = 5,\n",
    "                    batch_size=64)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps_1=tuner.get_best_hyperparameters(num_trials=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "FiWHBeDhnLq2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FiWHBeDhnLq2",
    "outputId": "e6716705-63c7-4025-e1d4-0e5a45aaee88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "85/85 [==============================] - 72s 827ms/step - loss: 1.5200 - accuracy: 0.3326 - val_loss: 1.2666 - val_accuracy: 0.4322\n",
      "Epoch 2/5\n",
      "85/85 [==============================] - 106s 1s/step - loss: 1.1151 - accuracy: 0.4917 - val_loss: 1.2074 - val_accuracy: 0.4683\n",
      "Epoch 3/5\n",
      "85/85 [==============================] - 72s 852ms/step - loss: 0.8633 - accuracy: 0.6396 - val_loss: 1.1751 - val_accuracy: 0.5006\n",
      "Epoch 4/5\n",
      "85/85 [==============================] - 72s 849ms/step - loss: 0.6134 - accuracy: 0.7659 - val_loss: 1.3092 - val_accuracy: 0.4656\n",
      "Epoch 5/5\n",
      "85/85 [==============================] - 73s 859ms/step - loss: 0.4191 - accuracy: 0.8565 - val_loss: 1.5234 - val_accuracy: 0.4639\n",
      "Best epoch: 3\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 5 epochs\n",
    "model = tuner.hypermodel.build(best_hps_1)\n",
    "history = model.fit(padded_sequence_train, y_train, epochs=5,batch_size=64, validation_data=(padded_sequence_vali, y_vali))\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "085adc9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "085adc9b",
    "outputId": "aa8cf95e-56f5-4726-9cfb-25f53b49c36c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "128\n",
      "0.30000000000000004\n",
      "0\n",
      "0.001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add units_1,units_2 etc depending on the num_layers\n",
    "print(f\"\"\"\n",
    "{best_hps_1.get('units_LSTM')}\n",
    "{best_hps_1.get('rate')}\n",
    "{best_hps_1.get('num_layers')}\n",
    "{best_hps_1.get('learning_rate')}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2755a68",
   "metadata": {
    "id": "b2755a68"
   },
   "source": [
    "# 5. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945dc4d1",
   "metadata": {
    "id": "945dc4d1"
   },
   "source": [
    "## 5.1 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bf57f36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bf57f36",
    "outputId": "827f9deb-18bf-48f7-fbf4-55dee96d9948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Using Naive Bayes:  0.4816666666666667\n",
      "F1 Score: 0.4289962919683218\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayse Baseline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf.fit(tokenizer.sequences_to_texts_generator(padded_sequence_tv), y_tv_array)\n",
    "predictions = text_clf.predict(tokenizer.sequences_to_texts_generator(padded_sequence_test)) \n",
    "print('Accuracy Using Naive Bayes: ', (predictions == y_test_array).mean())\n",
    "print('F1 Score:', f1_score(y_test_array, predictions, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J7wRdH25cev4",
   "metadata": {
    "id": "J7wRdH25cev4"
   },
   "source": [
    "## 5.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6rsj6EHXcegu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6rsj6EHXcegu",
    "outputId": "7354f610-5b72-4b7c-8f1f-c6db675c5c8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 400, 128)          3037184   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 144)               157248    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 144)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               18560     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,213,637\n",
      "Trainable params: 3,213,637\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = 128, input_length=400))\n",
    "model.add(LSTM(144))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax')) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])  \n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "FDGwBrL8ceUU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDGwBrL8ceUU",
    "outputId": "8d75a51e-77b8-4239-ec22-3e42d965a006"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "113/113 [==============================] - 80s 697ms/step - loss: 1.4381 - accuracy: 0.3481 - val_loss: 1.2367 - val_accuracy: 0.4478\n",
      "Epoch 2/3\n",
      "113/113 [==============================] - 85s 750ms/step - loss: 1.0183 - accuracy: 0.5399 - val_loss: 1.1097 - val_accuracy: 0.5089\n",
      "Epoch 3/3\n",
      "113/113 [==============================] - 87s 774ms/step - loss: 0.7662 - accuracy: 0.6733 - val_loss: 1.2159 - val_accuracy: 0.4844\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded_sequence_tv, y_tv, validation_data=(padded_sequence_test, y_test),\n",
    "                    epochs = 3,\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8221c6",
   "metadata": {
    "id": "8c8221c6"
   },
   "source": [
    "# 6. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "I5eSq4IP_vlp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5eSq4IP_vlp",
    "outputId": "f1299c24-7efc-4933-de74-29b35ebd371c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 400, 128)          3037184   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 144)               157248    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 144)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               18560     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,213,637\n",
      "Trainable params: 3,213,637\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_best = Sequential() \n",
    "model_best.add(Embedding(input_dim = vocab_size, output_dim = 128, input_length=400))\n",
    "model_best.add(LSTM(144))\n",
    "model_best.add(Dropout(0.1))\n",
    "model_best.add(Dense(128, activation='relu'))\n",
    "model_best.add(Dense(5, activation='softmax')) \n",
    "\n",
    "model_best.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])  \n",
    "\n",
    "print(model_best.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2UtgW9lM_2Po",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UtgW9lM_2Po",
    "outputId": "2b23b04e-89f9-4d15-8e36-5dc2b1477477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "141/141 [==============================] - 93s 650ms/step - loss: 1.3490 - accuracy: 0.3827\n",
      "Epoch 2/2\n",
      "141/141 [==============================] - 100s 713ms/step - loss: 0.9750 - accuracy: 0.5730\n"
     ]
    }
   ],
   "source": [
    "history_best = model_best.fit(padded_sequence_realtrain, y_realtrain,\n",
    "                    epochs = 2,\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51fef157",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51fef157",
    "outputId": "aa00fe69-ad1a-49c3-d1c3-879eb38126d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       3\n",
       "1       3\n",
       "2       4\n",
       "3       5\n",
       "4       4\n",
       "       ..\n",
       "2995    3\n",
       "2996    4\n",
       "2997    1\n",
       "2998    5\n",
       "2999    5\n",
       "Length: 3000, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_best.predict(padded_sequence_realtest)\n",
    "df_pred = pd.DataFrame(y_pred, columns = [1,2,3,4,5])\n",
    "df_pred = df_pred.idxmax(axis=1)\n",
    "\n",
    "df_pred"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Task B (JX) Ver3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
